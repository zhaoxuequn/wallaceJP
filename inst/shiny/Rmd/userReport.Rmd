---
title: Wallace Session `r Sys.Date()`
output: html_document
mainfont: ipaexg.ttf
monofont: ipaexg.ttf
---

```{r setup, include=FALSE}
library(knitr)
#debug(knit)
knit_engines$set(asis = function(options) {
  #browser()
  if (options$echo && options$eval) knit_child(text = options$code)
})
knitr::opts_chunk$set(message = FALSE, warning = FALSE, eval = FALSE)
```

```{r vars, echo = FALSE, include = FALSE, eval = TRUE}
# comp1
occs.db <- rvs$comp1 == 'db'
occs.csv <- rvs$comp1 == 'csv'
occs.any <- occs.db | occs.db
# comp2
poccs.rem <- 'rem' %in% rvs$comp2
poccs.sel <- 'sel' %in% rvs$comp2
poccs.thin <- 'thin' %in% rvs$comp2
poccs.any <- poccs.rem | poccs.sel | poccs.thin
# comp3
env.bc <- rvs$comp3 == 'bc'
env.user <- rvs$comp3 == 'user'
env.any <- env.bc | env.user
# comp4
bg.Geodesic <- rvs$comp4.shp == 'bufDist'
bg.bb <- rvs$comp4.shp == 'bb'
bg.mcp <- rvs$comp4.shp == 'mcp'
bg.ptbuf <- rvs$comp4.shp == 'ptbuf'
bg.userCSV <- rvs$comp4.shp == 'csv'
bg.userShp <- rvs$comp4.shp == 'shp'
bg.user <- bg.userShp | bg.userCSV
bg.any <- rvs$comp4.shp != ''
bg.bufWid <- rvs$comp4.buf > 0
#bg.bufDist <- ifelse(is.na(rvs$comp4.bufDist), 0, rvs$comp4.bufDist)
bg.bufDist <- bgBufDist > 0
#bg.tgb <- rvs$bgTgbs == 'tgb'
bg.tgb <- if (!is.null(rvs$bgTgbs) && length(rvs$bgTgbs) > 0) {rvs$bgTgbs == 'tgb'} else {FALSE}
#bg.envmask <- rvs$bgTgbs == 'envmask'
bg.envmask <- if (!is.null(rvs$bgTgbs) && length(rvs$bgTgbs) > 0) {rvs$bgTgbs == 'envmask'} else {FALSE}
#bg.usertgb <- rvs$bgTgbs == 'userbg'
bg.usergp <- if (!is.null(rvs$bgTgbs) && length(rvs$bgTgbs) > 0) {rvs$bgTgbs == 'userbp'} else {FALSE}
# comp5
part.block <- rvs$comp5 == 'block'
part.cb1 <- rvs$comp5 == 'cb1'
part.cb2 <- rvs$comp5 == 'cb2'
part.jack <- rvs$comp5 == 'jack'
part.rand <- rvs$comp5 == 'rand'
part.any <- rvs$comp5 != ''
# comp6
mod.bioclim <- rvs$comp6 == 'bioclim'
mod.maxent <- rvs$comp6 == 'maxent'
mod.any <- mod.bioclim | mod.maxent
alg.maxent <- if (mod.maxent) {rvs$algMaxent == 'maxent.jar'} else {FALSE}
alg.maxnet <- if (mod.maxent) {rvs$algMaxent == 'maxnet'} else {FALSE}
# comp7
viz.pred.thr <- if (!(is.null(rvs$comp7.thr))) {!(rvs$comp7.thr == 'noThresh')} else {FALSE}
viz.pred.p10 <- rvs$comp7.thr == 'p10'
viz.pred.raw <- rvs$comp7.type == 'raw'
viz.pred.log <- rvs$comp7.type == 'logistic'
viz.pred.cll <- rvs$comp7.type == 'cloglog'
viz.pred <- 'map' %in% rvs$comp7
viz.mxEval <- 'mxEval' %in% rvs$comp7
viz.bcPlot <- 'bcPlot' %in% rvs$comp7
viz.resp <- 'resp' %in% rvs$comp7
viz.any <- rvs$comp7[length(rvs$comp7)] != ''
# comp8
pj.area <- rvs$comp8.pj == 'area'
pj.time <- rvs$comp8.pj == 'time'
pj.user <- rvs$comp8.pj == 'user'
pj.mess <- rvs$comp8.esim == 'mess'
pj.thr <- if (!(is.null(rvs$comp8.thr))) {!(rvs$comp8.thr == 'noThresh')} else {FALSE}
pj.any <- pj.area | pj.time | pj.user | pj.mess
```

*Wallace* v1.1.3-1 セッションのRコード履歴は以下の通りです。

この R Markdown ファイルを RStudio で実行することで、セッションの結果を再現することができます。

各コードブロックは "チャンク" と呼ばれ、RStudio の [ソース] ウィンドウの右上隅にある [実行] メニューでオプションを選択することで、1 つずつ、または一度にすべて実行できます。

詳細は<http://rmarkdown.rstudio.com>を参照してください

### パッケージのインストール

Wallace は以下の R パッケージを使用しています。
```{r loadPkgs}
library(spocc)
library(spThin)
library(ENMeval)
library(dplyr)
```

Wallaceには、異なるパッケージといくつかの追加機能を統合するために開発されたいくつかの関数も 含まれています。このため、ファイル`functions.R`をロードする必要があります。関数 'system.file()' がこのスクリプトを見つけ、'source()' がそれをロードします。

```{r loadFunctions}
source(system.file('shiny/original/funcs', 'functions.R', package = 'wallaceJP'))
```

## *`r "{{spName}}"`*の解析結果.

```{asis, echo = occs.any, eval = occs.any, include = occs.any}
### 在データを取得する
```

```{asis, echo = occs.db, eval = occs.db, include = occs.db}
The search for occurrences was limited to `r {{occNum}}` records. Obtain occurrence records of the selected species from the `r "{{dbName}}"` database.
```

```{r occSearch, echo = occs.db, include = occs.db}
# 選択したデータベースにオカレンスレコードを検索します
results <- spocc::occ(query = "{{spName}}", from = "{{dbName}}", limit = {{occNum}}, 
                      has_coords = TRUE)
# spoccオブジェクトからデータテーブルを取得します。
results.data <- results[["{{dbName}}"]]$data[[formatSpName("{{spName}}")]]
# 座標が重複する行を削除します。
occs.dups <- duplicated(results.data[c('longitude', 'latitude')])
occs <- results.data[!occs.dups,]
# 緯度と経度が数値であることを確認します (文字の場合もあります)。
occs$latitude <- as.numeric(occs$latitude)
occs$longitude <- as.numeric(occs$longitude)
# すべてのレコードに一意の ID を与える。
occs$occID <- row.names(occs)
```

```{asis, echo = occs.csv, eval = occs.csv, include = occs.csv}
User CSV path with occurrence data. If the CSV file is not in the current workspace, change to the correct file path (e.g. "/Users/darwin/Documents/occs.csv").
```

```{r occInput, echo = occs.csv}
# 注意：CSVファイルを含むフォルダーへのパスを指定してください。
d.occs <- ''
# ユーザー在データcsvファイルへのパスを作成します。
userOccs.path <- file.path(d.occs, "{{occsCSV}}")
# csvで読み込む
userOccs.csv <- read.csv(userOccs.path, header = TRUE)
# 座標が重複する行を削除します。
occs.dups <- duplicated(userOccs.csv[c('longitude', 'latitude')])
occs <- userOccs.csv[!occs.dups,]
# Nasを削除します
occs <- occs[complete.cases(occs$longitude, occs$latitude), ]
# すべてのレコードに一意の ID を与える。
occs$occID <- row.names(occs)
```




```{asis, echo = poccs.any, eval = poccs.any, include = poccs.any}
### 在データの処理
```

```{asis, echo = poccs.rem, eval = poccs.rem, include = poccs.rem}
Remove the occurrence localities with the following IDs: `r {{occsRemoved}}`.
```

```{r poccs.rem, echo = poccs.rem, include = poccs.rem}
# 選択された occID に一致する行を削除します
occs <- occs %>% filter(!(occID %in% {{occsRemoved}}))
```

```{asis, echo = poccs.sel, eval = poccs.sel, include = poccs.sel}
The following code recreates the polygon used to select occurrences to keep in the analysis.
```

```{r occsSelect, echo = poccs.sel, include = poccs.sel}
selCoords <- data.frame(x = {{occsSelX}}, y = {{occsSelY}})
selPoly <- sf::st_polygon(list(as.matrix(selCoords)))
# Create an sf object for the polygon
selPoly.sf <- sf::st_sfc(selPoly, crs = 4326)
# Extract coordinates from occs and convert to sf points
occs.xy <- occs[c('longitude', 'latitude')]
occs.sf <- sf::st_as_sf(occs.xy, coords = c("longitude", "latitude"), crs = 4326)
# Perform spatial intersection
intersect <- sf::st_intersects(occs.sf, selPoly.sf, sparse = FALSE)
# Extract row numbers of intersecting points
intersect.rowNums <- which(intersect)
# Subset occs based on intersection
occs <- occs[intersect.rowNums, ]
```

```{asis, echo = poccs.thin, eval = poccs.thin, include = poccs.thin}
Spatial thinning selected. Thin distance selected is `r {{thinDist}}` km.
```

```{r doThin, echo = poccs.thin, include = poccs.thin}
output <- spThin::thin(occs, 'latitude', 'longitude', 'name', thin.par = {{thinDist}}, reps = 100,
                        locs.thinned.list.return = TRUE, write.files = FALSE, verbose = FALSE)
```

```{asis, echo = poccs.thin, eval = poccs.thin, include = poccs.thin}
Since spThin did 100 iterations, there are 100 different variations of how it thinned your occurrence localities. As there is a stochastic element in the algorithm, some iterations may include more localities than the others, and we need to make sure we maximize the number of localities we proceed with.
```

```{r doThin2, echo = poccs.thin, include = poccs.thin}
# 最大出現回数を返す反復を見つける
maxThin <- which(sapply(output, nrow) == max(sapply(output, nrow)))
# 最大値が複数ある場合は、最初のものを選択します
maxThin <- output[[ifelse(length(maxThin) > 1, maxThin[1], maxThin)]]  
# 間引きされたoccのみに一致するようにoccをサブセットする
occs <- occs[as.numeric(rownames(maxThin)),]  
```




```{asis, echo = env.any, eval = env.any, include = env.any}
### 環境データ取得
```

``` {asis, echo = env.bc, eval = env.bc, include = env.bc}
Using WorldClim (http://www.worldclim.org/) bioclimatic dataset at resolution of `r {{bcRes}}` arcmin.
```

```{r getEnvBC, echo = env.bc, include = env.bc}
# WorldClim生物気候変数ラスターを取得する
path <- getwd()
envRes <- {{bcRes}}
if (envRes == 0.5) {
  envs <- geodata::worldclim_global(var = "bio", res = {{bcRes}}, lat = {{bcLat}}, lon = {{bcLon}}, path = path, version = "2.1")
} else {
  envs <- geodata::worldclim_global(var = "bio", res = {{bcRes}}, path = path, version = "2.1")
}

# 名前の変更ラスター変数
names(envs) <- gsub(".*_", "bio", names(envs))
i <- grep('bio[0-9]$', names(envs))
editNames <- paste('bio', sapply(strsplit(names(envs)[i], 'bio'), function(x) x[2]), sep='0')
names(envs)[i] <- editNames
# 選択した変数によるサブセット
envs <- envs[[{{bcSels}}]]
# occグリッドセルで環境値を抽出する
locs.vals <- terra::extract(envs[[1]], occs[, c('longitude', 'latitude')])
locs.vals <- locs.vals[, 1]
# 環境値のない occsを削除する
occs <- occs[!is.na(locs.vals), ]  
```

```{r getEnvUser, echo = env.user, include = env.user}
# 注意：CSVファイルへのフルパスを指定してください
d.envs <- ''
# ラスターファイルへのパスを作成する
userRas.paths <- file.path(d.envs, {{userEnvs}})
# ラスターファイルからラスタースタックを作成する
envs <- terra::rast(userRas.paths)
```




```{r bgSwitch, include = FALSE, eval = bg.any}
bgType <- switch("{{bgSel}}", 'bb'='Bounding Box', 'mcp'='Minimum Convex Polygon',   
                'ptsbuf'='Buffered Points',  'user'='User-defined',  'bufDist'='Point geodesic buffer')
```

```{asis, echo = bg.any, eval = bg.any, include = bg.any}
### 環境データの処理
Background selection technique chosen as `r bgType`.
```

```{r bgBB, echo = bg.bb, include = bg.bb}
xmin <- min(occs$longitude)
xmax <- max(occs$longitude)
ymin <- min(occs$latitude)
ymax <- max(occs$latitude)
bb <- matrix(c(xmin, xmin, xmax, xmax, xmin, ymin, ymax, ymax, ymin, ymin), ncol=2)
bgExt <- sf::st_polygon(list(bb))
bgExt.sf <- sf::st_sfc(bgExt, crs = 4326)
```

```{r bgMCP, echo = bg.mcp, include = bg.mcp}
occs.xy <- occs[c('longitude', 'latitude')]
#sp::coordinates(occs.xy) <- ~ longitude + latitude
bgExt <- mcp(occs.xy)
```

```{r bgPtBufs, echo = bg.ptbuf, include = bg.ptbuf}
# バッファリング用の空間点群を作成します
occs.xy <- occs[c('longitude', 'latitude')]
#sp::coordinates(occs.xy) <- ~ longitude + latitude
bgExt <- occs.xy
```

```{asis, echo = bg.userCSV, eval = bg.userCSV, include = bg.userCSV}
Read a .csv file and generate a Spatial Polygon object.
```

```{r bgUserCSV, echo = bg.userCSV, include = bg.userCSV}
# 注意：CSVファイルへのフルパスを指定してください
csvPath <- ''
# ポリゴンの座標でcsvを読み取る
shp <- read.csv(csvPath, header = TRUE)

bgExt <- sf::st_polygon(list(as.matrix(shp)))

bgExt.sf <- sf::st_sfc(bgExt, crs = 4326)
```

```{asis, echo = bg.userShp, eval = bg.userShp, include = bg.userShp}
User study extent name is `r "{{bgUserShpName}}"`. User study extent path is `r "{{bgUserShpPath}}"`. Read in the .shp file and generate a Spatial Polygon object.
```

```{r bg.userShp, echo = bg.userShp, include = bg.userShp}
# 注意: シェープファイルを含むフォルダへのパスを指定します
d.bg <- ''
# ポリゴンの座標でcsvを読み取る
#bgExt <- rgdal::readOGR(d.bg, "{{bgUserShpName}}")
bgExt <- terra::vect(paste0(d.bg, "{{bgUserShpName}}",".shp"))
```

```{asis, echo = bg.bufWid, eval = bg.bufWid, include = bg.bufWid}
Buffer size of the study extent polygon defined as `r {{bgBuf}}` degrees.
```

```{asis, echo = bg.bufDist & bg.Geodesic, eval = bg.bufDist , include = bg.bufDist}
Buffer size of the study extent polygon defined as `r {{bgBufDist}}` km.
```

```{r bgufWid, echo = bg.bufWid & bg.ptbuf, inclue = bg.bufWid}
#bgExt <- rgeos::gBuffer(bgExt, width = {{bgBuf}})
eckertIV <- "+proj=eck4 +lon_0=0 +x_0=0 +y_0=0 +datum=WGS84 +units=m +no_defs"
occs.sp <- sf::st_as_sf(bgExt, coords = c("longitude", "latitude"), crs = 4326)
occs.sf <- sf::st_transform(occs.sp, crs = eckertIV)
bgExt <- sf::st_buffer(occs.sf, dist = {{bgBuf}} * 111000) |>
  sf::st_union() |>
  sf::st_sf() |>
  sf::st_transform(crs = 4326)
```

```{r bgufWid1, echo = bg.bufWid & bg.mcp, include = bg.bufWid}
#bgExt <- rgeos::gBuffer(bgExt, width = {{bgBuf}})
bgExt <- terra::buffer(terra::vect(bgExt), width = {{bgBuf}})
```

```{r bgufWid2, echo = bg.bufWid & (bg.bb | bg.userCSV), include = bg.bufWid}
bgExt_vect <- sf::st_transform(bgExt.sf , crs = 3100)
bgExt <- sf::st_buffer(bgExt_vect, dist = {{bgBuf}} * 111000)|>
  sf::st_transform(crs = 4326)|>
  terra::vect()
```

```{r bgufWid3, echo = bg.bufWid & bg.userShp, include = bg.bufWid}
#bgExt <- rgeos::gBuffer(bgExt, width = {{bgBuf}})
bgExt <- terra::buffer(bgExt, width = {{bgBuf}})
```

```{asis, echo = bg.any, eval = bg.any, include = bg.any}
Mask environmental variables by `r bgType`, and take a random sample of background values from the study extent. As the sample is random, your results may be different than those in the session. If there seems to be too much variability in these background samples, try increasing the number from 10,000 to something higher (e.g. 50,000 or 100,000). The better your background sample, the less variability you'll have between runs.
```

```{r bgGeodesic, echo = bg.Geodesic, include = bg.Geodesic}
occs.xy <- occs[c('longitude', 'latitude')]
occs.sf <- sf::st_as_sf(occs.xy, coords = c("longitude", "latitude"), crs = 4326)
#JGD2000 / UTM zone 54N - EPSG:3100
occs.sf <- sf::st_transform(occs.sf, crs = 3100)

bgExt <- sf::st_buffer(occs.sf, dist = {{bgBufDist}} * 1000) %>%
  sf::st_sf() %>%
  sf::st_transform(crs = 4326)
```

```{r envsMask, echo = bg.any, include = bg.any}
# 背景範囲の形状で環境ラスターをトリミングします
envsBgCrop <- terra::crop(envs, bgExt)
# トリミングされたラスターから背景範囲の形状をマスクします
envsBgMsk <- terra::mask(envsBgCrop, bgExt)
```

```{r tgbGbif, echo = bg.tgb, include = bg.tgb}
# 対象群背景法による背景ポイントのサンプル(対象種と同属の在データをGBIFから抽出)
tgb_bbox <- sf::st_bbox(bgExt)
taxonKey <- rgbif::name_backbone(name = occs$genus[1], rank = "genus")$usageKey
tgb <- rgbif::occ_search(
       taxonKey = taxonKey,
       hasCoordinate = TRUE,
       limit = {{bgPtsNum}} *4,
       decimalLatitude = paste(tgb_bbox[[2]], tgb_bbox[[4]], sep = ","),
       decimalLongitude = paste(tgb_bbox[[1]], tgb_bbox[[3]], sep = ","),
       fields = c("decimalLongitude", "decimalLatitude")
   )
tgb.xy <- tgb$data %>% dplyr::select(decimalLongitude, decimalLatitude) %>%
  dplyr::filter(!is.na(decimalLatitude) & !is.na(decimalLongitude))
names(tgb.xy) <- c('longitude', 'latitude')
tgb.xy <- as.data.frame(tgb.xy) 
tgb.xy <- tgb.xy[!duplicated(tgb.xy),]
bgExt.sf <- sf::st_as_sf(bgExt)
sf::st_crs(bgExt.sf) <- 4326
tgb.xy <- tgb.xy[sf::st_intersects(bgExt.sf, sf::st_as_sf(tgb.xy,coords = c("longitude", "latitude"), crs = 4326))[[1]],]
if(dim(tgb.xy)[1] > {{bgPtsNum}}){
   bgXY <- dplyr::sample_n(tbl = tgb.xy,size = {{bgPtsNum}})
} else {
   bgXY <- tgb.xy
}
bg.xy <- bgXY
```


```{r tgbEnvMask, echo = (bg.envmask | bg.user), include = bg.envmask}
# ランダムな背景ポイントのサンプリング
bg.xy <- terra::spatSample(envsBgMsk, size = {{bgPtsNum}},method = "random", xy = TRUE)
# 行列出力をデータフレームに変換します
bg.xy <- as.data.frame(bg.xy)[,c("x","y")]
colnames(bg.xy) <- c("longitude", "latitude")
```

```{r UsergpCsv, echo = bg.usergp, include = bg.usergp}
# ユーザー指定の背景点
# 注意：CSVファイルを含むフォルダーへのパスを指定してください。
d.user_gp <- ''
# 背景点の座標でcsvを読み取る
# input CSV file with columns 'longitude', 'latitude'
f.xy <- read.csv(d.user_gp, header = TRUE)
f.xy$latitude <- as.numeric(f.xy$latitude)
f.xy$longitude <- as.numeric(f.xy$longitude)
f.xy <- f.xy[c('longitude', 'latitude')]
f.xy <- as.data.frame(f.xy)
colnames(f.xy) <- c("longitude", "latitude")
terra::crs(bgExt) <- "EPSG:4326"
f.xy <- f.xy[!is.na(f.xy$longitude) & !is.na(f.xy$latitude),]
bg.xy <- f.xy[sf::st_intersects(sf::st_as_sf(bgExt,crs = 4326),sf::st_as_sf(f.xy,coords = c("longitude", "latitude"),crs = 4326))[[1]],]
```


```{r partSwitch, include = FALSE, eval = part.any}
partSwitch <- switch("{{partSel}}", 'block'='Block', 'cb1'='Checkerboard 1', 'cb2'='Checkerboard 2', 'jack'='Jackknife', 'random'='Random')
```

```{asis, echo = part.any, eval = part.any, include = part.any}
### 在データの分割
Occurrence data is now partitioned for cross-validation, a method that iteratively builds a model on all but one group and evaluates that model on the left-out group. 

For example, if the data is partitioned into 3 groups A, B, and C, a model is first built with groups A and B and is evaluated on C. This is repeated by building a model with B and C and evaluating on A, and so on until all combinations are done. 

Cross-validation operates under the assumption that the groups are independent of each other, which may or may not be a safe assumption for your dataset. Spatial partitioning is one way to ensure more independence between groups.

You selected to partition your occurrence data by the `r partSwitch` method.
```

```{r blockGrp, echo = part.block, include = part.block}
occs.xy <- occs[c('longitude', 'latitude')]
group.data <- ENMeval::get.block(occ = occs.xy, bg = bg.xy)
```

```{r check1Grp, echo = part.cb1, include = part.cb1}
occs.xy <- occs[c('longitude', 'latitude')]
group.data <- ENMeval::get.checkerboard(occ = occs.xy, env = envsBgMsk,
                                         bg = bg.xy, aggregation.factor = {{aggFact}})
```

```{r check2Grp, echo = part.cb2, include = part.cb2}
occs.xy <- occs[c('longitude', 'latitude')]
bg.xy.clean <- bg.xy[complete.cases(terra::extract(envsBgMsk, bg.xy)), ]
occs.xy.clean <- occs.xy[complete.cases(terra::extract(envsBgMsk, occs.xy)), ]
group.data <- ENMeval::get.checkerboard(occ = occs.xy.clean, env = envsBgMsk,
                                         bg = bg.xy.clean, aggregation.factor = {{aggFact}})
```

```{r jackGrp, echo = part.jack, include = part.jack}
occs.xy <- occs[c('longitude', 'latitude')]
bg.xy.clean <- bg.xy[complete.cases(terra::extract(envsBgMsk, bg.xy)), ]
occs.xy.clean <- occs.xy[complete.cases(terra::extract(envsBgMsk, occs.xy)), ]
group.data <- ENMeval::get.jackknife(occ = occs.xy.clean, bg = bg.xy.clean)
```

```{r randGrp, echo = part.rand, include = part.rand}
occs.xy <- occs[c('longitude', 'latitude')]
bg.xy.clean <- bg.xy[complete.cases(terra::extract(envsBgMsk, bg.xy)), ]
occs.xy.clean <- occs.xy[complete.cases(terra::extract(envsBgMsk, occs.xy)), ]
group.data <- ENMeval::get.randomkfold(occ = occs.xy.clean, bg = bg.xy.clean, kfolds = {{kfolds}})
```

```{r  echo = part.any, include = part.any}
# リストから在データと背景分割グループ番号を引き出します
occs.grp <- group.data[[1]]
bg.grp <- group.data[[2]]
```




```{asis, echo = mod.any, eval = mod.any, include = mod.any}
### ニッチモデルの構築と評価
You selected the `r "{{enmSel}}"` model.
```

```{r bioclim, echo = mod.bioclim, include = mod.bioclim}
e <- ENMeval::ENMevaluate(occs = as.data.frame(occs.xy.clean), envs = envsBgMsk, bg = bg.xy.clean,
                          algorithm = "bioclim", partitions = "user",
                          user.grp = list(occs.grp = occs.grp, bg.grp = bg.grp))
# 結果データフレームとモデルのリストを解凍します
evalTbl <- e@results
evalMods <- e@models
names(e@predictions) <- "bioclim"
evalPreds <- e@predictions
```

```{r maxent, echo = mod.maxent, include = mod.maxent, results = 'hide'}
# テストする正則化乗数のベクトルを定義します
rms <- seq({{rms1}}, {{rms2}}, {{rmsStep}})
# 選択したすべてのパラメータ設定に対してモデル構築を反復処理する
e <- ENMeval::ENMevaluate(occs = as.data.frame(occs.xy.clean), envs = envsBgMsk, bg = bg.xy.clean,
                          tune.args = list(fc = {{fcs}}, rm = rms), partitions = 'user', 
                          user.grp = list(occs.grp = occs.grp, bg.grp = bg.grp), 
                          algorithm = "{{algMaxent}}")

# 結果データフレーム、モデルのリスト、および生の予測のRasterStackを解凍します
evalTbl <- e@results
evalMods <- e@models
names(evalMods) <- e@tune.settings$tune.args
evalPreds <- e@predictions
```

```{asis, echo = viz.any, eval = viz.any, include = viz.any}
### ニッチモデルを視覚化する
Below are some visualizations of your modeling analysis results.
```

```{r respCurvJav, echo = viz.resp & '{{algMaxent}}' == "maxent.jar", include = viz.resp}
# 係数がゼロ以外の環境変数の応答曲線を表示する
predicts::partialResponse(evalMods[["{{modSel}}"]], var = {{mxNonZeroCoefs}})
```

```{r respCurvMaxnet, echo = viz.resp & '{{algMaxent}}' == "maxnet", include = viz.resp, warning = F, message = F}
# 係数がゼロ以外の環境変数の応答曲線を表示する
plot(evalMods[["{{modSel}}"]], vars = {{mxNonZeroCoefs}}, type = "cloglog")
```

```{r bcPlot, echo = viz.bcPlot, include = viz.bcPlot}
# BIOCLIMエンベローププロットを表示
plot(evalMods[['bioclim']], a = {{bcPlot1}}, b = {{bcPlot2}}, p = {{bcPlotP}})
```

```{r evalPlot, echo = viz.mxEval, include = viz.mxEval}
# ENMevalの結果を表示する
ENMeval::evalplot.stats(e, stats = "{{mxEvalSel}}", "rm", "fc")
```

```{r mapPred, echo = viz.pred, include = viz.pred}
# モデルリストからモデルを選択します。
mod <- evalMods[["{{modSel}}"]]
```

```{r, echo = viz.pred & mod.bioclim, include = viz.pred}
# 生物気候予測を生成する
pred <- evalPreds[["{{modSel}}"]]
```

```{r, echo = viz.pred.raw, include = viz.pred.raw}
# 生の予測を生成する
pred <- evalPreds[["{{modSel}}"]]
```

```{r maxnetLog, echo = viz.pred.log & alg.maxnet, include = viz.pred.log}
# ロジスティック予測の生成
pred <- predictMaxnet(mod, envsBgMsk, type = 'logistic', clamp = {{clamp}})
```

```{r maxnetCll, echo = viz.pred.cll & alg.maxnet, include = viz.pred.cll}
# cloglog予測を生成する
pred <- predictMaxnet(mod, envsBgMsk, type = 'cloglog', clamp = {{clamp}}) 
```

```{r maxentLog, echo = viz.pred.log & alg.maxent, include = viz.pred.log}
# ロジスティック予測の生成
pred <- predicts::predict(envsBgMsk, mod, args = c("outputformat=logistic"), clamp = rvs$clamp, na.rm = TRUE)
```

```{r maxentCll, echo = viz.pred.cll & alg.maxent, include = viz.pred.cll}
# cloglog予測を生成する
pred <- predicts::predict(envsBgMsk, mod, args = c("outputformat=cloglog"), clamp = rvs$clamp, na.rm = TRUE) 
```

```{r, echo = viz.pred.thr, include = viz.pred.thr}
# 在データグリッドセルの予測値を取得する
occPredVals <- terra::extract(pred, occs.xy)
# 最小トレーニングプレゼンスしきい値を定義する
thr <- thresh(occPredVals[[2]], "{{comp7.thresh}}")
# しきい値モデルの予測
pred <- pred > thr
```

```{r, echo = viz.pred, include = viz.pred}
# モデル予測をプロットする
terra::plot(pred)
```

```{asis, echo = pj.any, eval = pj.any, include = pj.any}
### ニッチモデルの投影
You selected to project your model. First define a polygon with the coordinates you chose, then crop and mask your predictor rasters. Finally, predict suitability values for these new raster cells based on the model you selected.
```

```{r projSel, echo = pj.any, include = pj.any}
projCoords <- data.frame(x = {{occsPjX}},
                         y = {{occsPjY}})

projPoly <- sf::as_Spatial(sf::st_sfc(sf::st_polygon(list(as.matrix(projCoords))), crs = 4326))
```

```{asis, echo = pj.area & alg.maxnet, eval = pj.area, include = pj.area}
### ニッチモデルを新しい空間に投影する
Now use crop and mask the predictor variables by projPoly, and use the maxnet.predictRaster() function to predict the values for the new extent based on the model selected.
```

```{r projAreaMaxnet.raw, echo = pj.area & alg.maxnet & viz.pred.raw, include = pj.area}
predsProj <- terra::crop(envs, projPoly)
predsProj <- terra::mask(predsProj, terra::vect(sf::st_as_sf(projPoly)))
proj <- predictMaxnet(mod, predsProj, type = 'exponential', clamp = {{clamp}})
```

```{r projAreaMaxnet.log, echo = pj.area & alg.maxnet & viz.pred.log, include = pj.area}
predsProj <- terra::crop(envs, projPoly)
predsProj <- terra::mask(predsProj, terra::vect(sf::st_as_sf(projPoly)))
proj <- predictMaxnet(mod, predsProj, type = 'logistic', clamp = {{clamp}})
```

```{r projAreaMaxnet.cll, echo = pj.area & alg.maxnet & viz.pred.cll, include = pj.area}
predsProj <- terra::crop(envs, projPoly)
predsProj <- terra::mask(predsProj, terra::vect(sf::st_as_sf(projPoly)))
proj <- predictMaxnet(mod, predsProj, type = 'cloglog', clamp = {{clamp}})
```

```{asis, echo = pj.area & alg.maxent, eval = pj.area, include = pj.area}
### ニッチモデルを新しい空間に投影する
Now use crop and mask the predictor variables by projPoly, and use the predicts::predict() function to predict the values for the new extent based on the model selected.
```

```{r projAreaMaxent.raw, echo = pj.area & alg.maxent & viz.pred.raw, include = pj.area}
predsProj <- terra::crop(envs, projPoly)
predsProj <- terra::mask(predsProj, terra::vect(sf::st_as_sf(projPoly)))
proj <- predicts::predict(predsProj, mod, args = c("outputformat=raw"),clamp = rvs$clamp, na.rm = TRUE)
```

```{r projAreaMaxent.log, echo = pj.area & alg.maxent & viz.pred.log, include = pj.area}
predsProj <- terra::crop(envs, projPoly)
predsProj <- terra::mask(predsProj, terra::vect(sf::st_as_sf(projPoly)))
proj <- predicts::predict(predsProj, mod, args = c("outputformat=logistic"), clamp = rvs$clamp, na.rm = TRUE)
```

```{r projAreaMaxent.cll, echo = pj.area & alg.maxent & viz.pred.cll, include = pj.area}
predsProj <- terra::crop(envs, projPoly)
predsProj <- terra::mask(predsProj, terra::vect(sf::st_as_sf(projPoly)))
proj <- predicts::predict(predsProj, mod, args = c("outputformat=cloglog"), clamp = rvs$clamp, na.rm = TRUE)
```

```{asis, echo = pj.area & mod.bioclim, eval = pj.area, include = pj.area}
### ニッチモデルを新しい空間に投影する
Now use crop and mask the predictor variables by projPoly, and use the predicts::predict() function to predict the values for the new extent based on the model selected.
```

```{r projAreaBioclim, echo = pj.area & mod.bioclim, include = pj.area}
predsProj <- terra::crop(envs, projPoly)
predsProj <- terra::mask(predsProj, terra::vect(sf::st_as_sf(projPoly)))
proj <- predicts::predict(predsProj, mod)
```

```{asis, echo = pj.time & alg.maxnet, eval = pj.time, include = pj.time}
### ニッチモデルを新しい時間に投影する
Now download the future climate variables chosen with *Wallace*, crop and mask them by projPoly, and use the maxnet.predictRaster() function to predict the values for the new time based on the model selected.
```

```{asis, echo = pj.time & alg.maxent, eval = pj.time, include = pj.time}
### ニッチモデルを新しい時間に投影する
Now download the future climate variables chosen with *Wallace*, crop and mask them by projPoly, and use the predicts::predict() function to predict the values for the new time based on the model selected.
```

```{asis, echo = pj.time & mod.bioclim, eval = pj.time, include = pj.time}
### ニッチモデルを新しい時間に投影する
Now download the future climate variables chosen with *Wallace*, crop and mask them by projPoly, and use the predicts::predict() function to predict the values for the new time based on the model selected.
```

```{r pTNet, echo = pj.time, include = pj.time}
envsFuture <- geodata::cmip6_world(var = "bio", res = {{bcRes}}, ssp = {{pjRCP}}, model = "{{pjGCM}}", time = "{{pjYear}}", path = getwd())

predsProj <- terra::crop(envsFuture, projPoly)
predsProj <- terra::mask(predsProj, terra::vect(sf::st_as_sf(projPoly)))

# 将来の気候変数名の名前を変更する
names(predsProj) <- paste0('bio', sprintf("%02d", 1:19))
# 気候変数を選択する
predsProj <- terra::subset(predsProj, names(envs))
```

```{r pTNet.raw, echo = pj.time & alg.maxnet & viz.pred.raw, include = pj.time}
# モデルを予測する
proj <- predictMaxnet(mod, predsProj, type = 'exponential', clamp = {{clamp}})
```

```{r pTNet.log, echo = pj.time & alg.maxnet & viz.pred.log, include = pj.time}
# モデルを予測する
proj <- predictMaxnet(mod, predsProj, type = 'logistic', clamp = {{clamp}})
```

```{r pTNet.cll, echo = pj.time & alg.maxnet & viz.pred.cll, include = pj.time}
# モデルを予測する
proj <- predictMaxnet(mod, predsProj, type = 'cloglog', clamp = {{clamp}})
```

```{r pTEnt.raw, echo = pj.time & alg.maxent & viz.pred.raw, include = pj.time}
# モデルを予測する
proj <- predicts::predict(predsProj, mod, args = c('outputformat=raw'), clamp = rvs$clamp, na.rm = TRUE)
```

```{r pTEnt.log, echo = pj.time & alg.maxent & viz.pred.log, include = pj.time}
# モデルを予測する
proj <- predicts::predict(predsProj, mod, args = c('outputformat=logistic'), clamp = rvs$clamp, na.rm = TRUE)
```

```{r pTEnt.cll, echo = pj.time & alg.maxent & viz.pred.cll, include = pj.time}
# モデルを予測する
proj <- predicts::predict(predsProj, mod, args = c('outputformat=cloglog'), clamp = rvs$clamp, na.rm = TRUE)
```

```{r pTBioclim, echo = pj.time & mod.bioclim, include = pj.time}
# モデルを予測する
proj <- predicts::predict(predsProj, mod)
```

```{asis, echo = pj.user & alg.maxnet, eval = pj.user, include = pj.user}
### ニッチモデルを新しい時間に投影する
Now download the future climate variables chosen with *Wallace*, crop and mask them by projPoly, and use the maxnet.predictRaster() function to predict the values for the new time based on the model selected.
```

```{asis, echo = pj.user & alg.maxent, eval = pj.user, include = pj.user}
### ニッチモデルを新しい時間に投影する
Now download the future climate variables chosen with *Wallace*, crop and mask them by projPoly, and use the predicts::predict() function to predict the values for the new time based on the model selected.
```

```{asis, echo = pj.user & mod.bioclim, eval = pj.user, include = pj.user}
### ニッチモデルを新しい時間に投影する
Now download the future climate variables chosen with *Wallace*, crop and mask them by projPoly, and use the predicts::predict() function to predict the values for the new time based on the model selected.
```

```{r getprojectUserEnvs, echo = pj.user, include = pj.user}
# 注意：ラスターァイルへのフルパスを指定してください
d.penvs <- ''
# ラスターファイルへのパスを作成する
pUserRas.paths <- file.path(d.penvs, {{rvs$projectUserEnvs[1]}})
# ラスターファイルからラスタースタックを作成する
pUserEnvs <- terra::rast(pUserRas.paths)

predsProj <- terra::crop(pUserEnvs, projPoly)
predsProj <- terra::mask(predsProj, terra::vect(sf::st_as_sf(projPoly)))
```

```{r pUNet.raw, echo = pj.user & alg.maxnet & viz.pred.raw, include = pj.user}
# モデルを予測する
proj <- predictMaxnet(mod, predsProj, type = 'exponential', clamp = {{clamp}})
```

```{r pUNet.log, echo = pj.user & alg.maxnet & viz.pred.log, include = pj.user}
# モデルを予測する
proj <- predictMaxnet(mod, predsProj, type = 'logistic', clamp = {{clamp}})
```

```{r pUNet.cll, echo = pj.user & alg.maxnet & viz.pred.cll, include = pj.user}
# モデルを予測する
proj <- predictMaxnet(mod, predsProj, type = 'cloglog', clamp = {{clamp}})
```

```{r pUEnt.raw, echo = pj.user & alg.maxent & viz.pred.raw, include = pj.user}
# モデルを予測する
proj <- predicts::predict(predsProj, mod, args = c('outputformat=raw'), clamp = rvs$clamp, na.rm = TRUE)
```

```{r pUEnt.log, echo = pj.user & alg.maxent & viz.pred.log, include = pj.user}
# モデルを予測する
proj <- predicts::predict(predsProj, mod, args = c('outputformat=logistic'), clamp = rvs$clamp, na.rm = TRUE)
```

```{r pUEnt.cll, echo = pj.user & alg.maxent & viz.pred.cll, include = pj.user}
# モデルを予測する
proj <- predicts::predict(predsProj, mod, args = c('outputformat=cloglog'), clamp = rvs$clamp, na.rm = TRUE)
```

```{r pUBioclim, echo = pj.user & mod.bioclim, include = pj.user}
# モデルを予測する
proj <- predicts::predict(predsProj, mod)
```

```{r, echo = pj.thr, include = pj.thr}
# 在データグリッドセルの予測値を取得する
# occPredVals <- terra::extract(pred, occs.xy)
# # 最小トレーニングプレゼンスしきい値を定義する
# thr <- thresh(occPredVals, "{{comp8.thresh}}")
# しきい値モデルの予測
proj <- proj > thr
```

```{r, echo = pj.any, include = pj.any}
# モデル予測をプロットする
terra::plot(proj)
```

```{asis, echo = pj.mess, eval = pj.mess, include = pj.mess}
### 環境類似度の計算
To visualize the environmental difference between the occurrence localities and your selected projection extent, calculate a multidimensional environmental similarity surface (MESS). High negative values mean great difference, whereas high positive values mean great similarity. Interpreting the projected suitability for areas with high negative values should be done with extreme caution, as they are outside the environmental range of the occurrence localities.
```

```{r projMESS, echo = pj.mess, include = pj.mess}

# 在地点と背景から環境値を抽出します-これらはモデルに入った値でした
names(bg.xy) <- names(occs.xy)
all.xy <- rbind(occs.xy, bg.xy)
occEnvVals <- terra::extract(envs, all.xy, ID=FALSE)
# これらの値を投影範囲（envsMsk）と比較します
proj.mess <- suppressWarnings(predicts::mess(predsProj, occEnvVals))
terra::plot(proj.mess)
```
